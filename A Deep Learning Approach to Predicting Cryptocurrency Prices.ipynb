{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # A Deep Learning Approach to Predicting Cryptocurrency Prices (Ethereum, Bitcoin, etc.)\n",
    " \n",
    "##### We will Implement a recurrent neural network to predict bitcoin prices\n",
    "\n",
    "![alt text](https://dashee87.github.io/images/bitcoin_ether_training_test.png \"Logo Title Text 1\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Keras for deep learning\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers import Bidirectional\n",
    "from keras.models import Sequential\n",
    "\n",
    "## Scikit learn for mapping metrics\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "#for logging\n",
    "import time\n",
    "\n",
    "##matrix math\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "##plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "##data processing\n",
    "import pandas as pd\n",
    "\n",
    "##Talking to gdax's api\n",
    "import gdax\n",
    "\n",
    "##Current time\n",
    "import time\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "columns = [\"time\", \"low\", \"high\", \"open\", \"close\", \"volume\"]\n",
    "public_client = gdax.PublicClient()\n",
    "\n",
    "end_time = int(time.time())\n",
    "# end_time = 1516573080\n",
    "end_time -= end_time % 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end time   1516759020\n",
      "start time 1516747020\n",
      "\n",
      "end time   1516747020\n",
      "start time 1516735020\n",
      "\n",
      "end time   1516735020\n",
      "start time 1516723020\n",
      "\n",
      "end time   1516723020\n",
      "start time 1516711020\n",
      "\n",
      "end time   1516711020\n",
      "start time 1516699020\n",
      "\n",
      "end time   1516699020\n",
      "start time 1516687020\n",
      "\n",
      "end time   1516687020\n",
      "start time 1516675020\n",
      "\n",
      "end time   1516675020\n",
      "start time 1516663020\n",
      "\n",
      "end time   1516663020\n",
      "start time 1516651020\n",
      "\n",
      "end time   1516651020\n",
      "start time 1516639020\n",
      "\n",
      "end time   1516639020\n",
      "start time 1516627020\n",
      "\n",
      "end time   1516627020\n",
      "start time 1516615020\n",
      "\n",
      "end time   1516615020\n",
      "start time 1516603020\n",
      "\n",
      "end time   1516603020\n",
      "start time 1516591020\n",
      "\n",
      "end time   1516591020\n",
      "start time 1516579020\n",
      "\n",
      "end time   1516579020\n",
      "start time 1516567020\n",
      "\n",
      "end time   1516567020\n",
      "start time 1516555020\n",
      "\n",
      "end time   1516555020\n",
      "start time 1516543020\n",
      "\n",
      "end time   1516543020\n",
      "start time 1516531020\n",
      "\n",
      "end time   1516531020\n",
      "start time 1516519020\n",
      "\n",
      "end time   1516519020\n",
      "start time 1516507020\n",
      "\n",
      "end time   1516507020\n",
      "start time 1516495020\n",
      "\n",
      "end time   1516495020\n",
      "start time 1516483020\n",
      "\n",
      "end time   1516483020\n",
      "start time 1516471020\n",
      "\n",
      "end time   1516471020\n",
      "start time 1516459020\n",
      "\n",
      "end time   1516459020\n",
      "start time 1516447020\n",
      "\n",
      "end time   1516447020\n",
      "start time 1516435020\n",
      "\n",
      "end time   1516435020\n",
      "start time 1516423020\n",
      "\n",
      "end time   1516423020\n",
      "start time 1516411020\n",
      "\n",
      "end time   1516411020\n",
      "start time 1516399020\n",
      "\n",
      "end time   1516399020\n",
      "start time 1516387020\n",
      "\n",
      "end time   1516387020\n",
      "start time 1516375020\n",
      "\n",
      "end time   1516375020\n",
      "start time 1516363020\n",
      "\n",
      "end time   1516363020\n",
      "start time 1516351020\n",
      "\n",
      "end time   1516351020\n",
      "start time 1516339020\n",
      "\n",
      "end time   1516339020\n",
      "start time 1516327020\n",
      "\n",
      "end time   1516327020\n",
      "start time 1516315020\n",
      "\n",
      "end time   1516315020\n",
      "start time 1516303020\n",
      "\n",
      "end time   1516303020\n",
      "start time 1516291020\n",
      "\n",
      "end time   1516291020\n",
      "start time 1516279020\n",
      "\n",
      "end time   1516279020\n",
      "start time 1516267020\n",
      "\n",
      "end time   1516267020\n",
      "start time 1516255020\n",
      "\n",
      "end time   1516255020\n",
      "start time 1516243020\n",
      "\n",
      "end time   1516243020\n",
      "start time 1516231020\n",
      "\n",
      "end time   1516231020\n",
      "start time 1516219020\n",
      "\n",
      "end time   1516219020\n",
      "start time 1516207020\n",
      "\n",
      "end time   1516207020\n",
      "start time 1516195020\n",
      "\n",
      "end time   1516195020\n",
      "start time 1516183020\n",
      "\n",
      "end time   1516183020\n",
      "start time 1516171020\n",
      "\n",
      "end time   1516171020\n",
      "start time 1516159020\n",
      "\n",
      "end time   1516159020\n",
      "start time 1516147020\n",
      "\n",
      "end time   1516147020\n",
      "start time 1516135020\n",
      "\n",
      "end time   1516135020\n",
      "start time 1516123020\n",
      "\n",
      "end time   1516123020\n",
      "start time 1516111020\n",
      "\n",
      "end time   1516111020\n",
      "start time 1516099020\n",
      "\n",
      "end time   1516099020\n",
      "start time 1516087020\n",
      "\n",
      "end time   1516087020\n",
      "start time 1516075020\n",
      "\n",
      "end time   1516075020\n",
      "start time 1516063020\n",
      "\n",
      "end time   1516063020\n",
      "start time 1516051020\n",
      "\n",
      "end time   1516051020\n",
      "start time 1516039020\n",
      "\n",
      "end time   1516039020\n",
      "start time 1516027020\n",
      "\n",
      "end time   1516027020\n",
      "start time 1516015020\n",
      "\n",
      "end time   1516015020\n",
      "start time 1516003020\n",
      "\n",
      "end time   1516003020\n",
      "start time 1515991020\n",
      "\n",
      "end time   1515991020\n",
      "start time 1515979020\n",
      "\n",
      "end time   1515979020\n",
      "start time 1515967020\n",
      "\n",
      "end time   1515967020\n",
      "start time 1515955020\n",
      "\n",
      "end time   1515955020\n",
      "start time 1515943020\n",
      "\n",
      "end time   1515943020\n",
      "start time 1515931020\n",
      "\n",
      "end time   1515931020\n",
      "start time 1515919020\n",
      "\n",
      "end time   1515919020\n",
      "start time 1515907020\n",
      "\n",
      "end time   1515907020\n",
      "start time 1515895020\n",
      "\n",
      "end time   1515895020\n",
      "start time 1515883020\n",
      "\n",
      "end time   1515883020\n",
      "start time 1515871020\n",
      "\n",
      "end time   1515871020\n",
      "start time 1515859020\n",
      "\n",
      "end time   1515859020\n",
      "start time 1515847020\n",
      "\n",
      "end time   1515847020\n",
      "start time 1515835020\n",
      "\n",
      "end time   1515835020\n",
      "start time 1515823020\n",
      "\n",
      "end time   1515823020\n",
      "start time 1515811020\n",
      "\n",
      "end time   1515811020\n",
      "start time 1515799020\n",
      "\n",
      "end time   1515799020\n",
      "start time 1515787020\n",
      "\n",
      "end time   1515787020\n",
      "start time 1515775020\n",
      "\n",
      "end time   1515775020\n",
      "start time 1515763020\n",
      "\n",
      "end time   1515763020\n",
      "start time 1515751020\n",
      "\n",
      "end time   1515751020\n",
      "start time 1515739020\n",
      "\n",
      "end time   1515739020\n",
      "start time 1515727020\n",
      "\n",
      "end time   1515727020\n",
      "start time 1515715020\n",
      "\n",
      "end time   1515715020\n",
      "start time 1515703020\n",
      "\n",
      "end time   1515703020\n",
      "start time 1515691020\n",
      "\n",
      "end time   1515691020\n",
      "start time 1515679020\n",
      "\n",
      "end time   1515679020\n",
      "start time 1515667020\n",
      "\n",
      "end time   1515667020\n",
      "start time 1515655020\n",
      "\n",
      "end time   1515655020\n",
      "start time 1515643020\n",
      "\n",
      "end time   1515643020\n",
      "start time 1515631020\n",
      "\n",
      "end time   1515631020\n",
      "start time 1515619020\n",
      "\n",
      "end time   1515619020\n",
      "start time 1515607020\n",
      "\n",
      "end time   1515607020\n",
      "start time 1515595020\n",
      "\n",
      "end time   1515595020\n",
      "start time 1515583020\n",
      "\n",
      "end time   1515583020\n",
      "start time 1515571020\n",
      "\n",
      "end time   1515571020\n",
      "start time 1515559020\n",
      "\n",
      "end time   1515559020\n",
      "start time 1515547020\n",
      "\n",
      "end time   1515547020\n",
      "start time 1515535020\n",
      "\n",
      "end time   1515535020\n",
      "start time 1515523020\n",
      "\n",
      "end time   1515523020\n",
      "start time 1515511020\n",
      "\n",
      "end time   1515511020\n",
      "start time 1515499020\n",
      "\n",
      "end time   1515499020\n",
      "start time 1515487020\n",
      "\n",
      "end time   1515487020\n",
      "start time 1515475020\n",
      "\n",
      "end time   1515475020\n",
      "start time 1515463020\n",
      "\n",
      "end time   1515463020\n",
      "start time 1515451020\n",
      "\n",
      "end time   1515451020\n",
      "start time 1515439020\n",
      "\n",
      "end time   1515439020\n",
      "start time 1515427020\n",
      "\n",
      "end time   1515427020\n",
      "start time 1515415020\n",
      "\n",
      "end time   1515415020\n",
      "start time 1515403020\n",
      "\n",
      "end time   1515403020\n",
      "start time 1515391020\n",
      "\n",
      "end time   1515391020\n",
      "start time 1515379020\n",
      "\n",
      "end time   1515379020\n",
      "start time 1515367020\n",
      "\n",
      "end time   1515367020\n",
      "start time 1515355020\n",
      "\n",
      "end time   1515355020\n",
      "start time 1515343020\n",
      "\n",
      "end time   1515343020\n",
      "start time 1515331020\n",
      "\n",
      "end time   1515331020\n",
      "start time 1515319020\n",
      "\n",
      "end time   1515319020\n",
      "start time 1515307020\n",
      "\n",
      "end time   1515307020\n",
      "start time 1515295020\n",
      "\n",
      "end time   1515295020\n",
      "start time 1515283020\n",
      "\n",
      "end time   1515283020\n",
      "start time 1515271020\n",
      "\n",
      "end time   1515271020\n",
      "start time 1515259020\n",
      "\n",
      "end time   1515259020\n",
      "start time 1515247020\n",
      "\n",
      "end time   1515247020\n",
      "start time 1515235020\n",
      "\n",
      "end time   1515235020\n",
      "start time 1515223020\n",
      "\n",
      "end time   1515223020\n",
      "start time 1515211020\n",
      "\n",
      "end time   1515211020\n",
      "start time 1515199020\n",
      "\n",
      "end time   1515199020\n",
      "start time 1515187020\n",
      "\n",
      "end time   1515187020\n",
      "start time 1515175020\n",
      "\n",
      "end time   1515175020\n",
      "start time 1515163020\n",
      "\n",
      "end time   1515163020\n",
      "start time 1515151020\n",
      "\n",
      "end time   1515151020\n",
      "start time 1515139020\n",
      "\n",
      "end time   1515139020\n",
      "start time 1515127020\n",
      "\n",
      "end time   1515127020\n",
      "start time 1515115020\n",
      "\n",
      "end time   1515115020\n",
      "start time 1515103020\n",
      "\n",
      "end time   1515103020\n",
      "start time 1515091020\n",
      "\n",
      "end time   1515091020\n",
      "start time 1515079020\n",
      "\n",
      "end time   1515079020\n",
      "start time 1515067020\n",
      "\n",
      "end time   1515067020\n",
      "start time 1515055020\n",
      "\n",
      "end time   1515055020\n",
      "start time 1515043020\n",
      "\n",
      "end time   1515043020\n",
      "start time 1515031020\n",
      "\n",
      "end time   1515031020\n",
      "start time 1515019020\n",
      "\n",
      "end time   1515019020\n",
      "start time 1515007020\n",
      "\n",
      "end time   1515007020\n",
      "start time 1514995020\n",
      "\n",
      "end time   1514995020\n",
      "start time 1514983020\n",
      "\n",
      "end time   1514983020\n",
      "start time 1514971020\n",
      "\n",
      "end time   1514971020\n",
      "start time 1514959020\n",
      "\n",
      "end time   1514959020\n",
      "start time 1514947020\n",
      "\n",
      "end time   1514947020\n",
      "start time 1514935020\n",
      "\n",
      "end time   1514935020\n",
      "start time 1514923020\n",
      "\n",
      "end time   1514923020\n",
      "start time 1514911020\n",
      "\n",
      "end time   1514911020\n",
      "start time 1514899020\n",
      "\n",
      "end time   1514899020\n",
      "start time 1514887020\n",
      "\n",
      "end time   1514887020\n",
      "start time 1514875020\n",
      "\n",
      "end time   1514875020\n",
      "start time 1514863020\n",
      "\n",
      "end time   1514863020\n",
      "start time 1514851020\n",
      "\n",
      "end time   1514851020\n",
      "start time 1514839020\n",
      "\n",
      "end time   1514839020\n",
      "start time 1514827020\n",
      "\n",
      "end time   1514827020\n",
      "start time 1514815020\n",
      "\n",
      "end time   1514815020\n",
      "start time 1514803020\n",
      "\n",
      "end time   1514803020\n",
      "start time 1514791020\n",
      "\n",
      "end time   1514791020\n",
      "start time 1514779020\n",
      "\n",
      "end time   1514779020\n",
      "start time 1514767020\n",
      "\n",
      "end time   1514767020\n",
      "start time 1514755020\n",
      "\n",
      "end time   1514755020\n",
      "start time 1514743020\n",
      "\n",
      "end time   1514743020\n",
      "start time 1514731020\n",
      "\n",
      "end time   1514731020\n",
      "start time 1514719020\n",
      "\n",
      "end time   1514719020\n",
      "start time 1514707020\n",
      "\n",
      "end time   1514707020\n",
      "start time 1514695020\n",
      "\n",
      "end time   1514695020\n",
      "start time 1514683020\n",
      "\n",
      "end time   1514683020\n",
      "start time 1514671020\n",
      "\n",
      "end time   1514671020\n",
      "start time 1514659020\n",
      "\n",
      "end time   1514659020\n",
      "start time 1514647020\n",
      "\n",
      "end time   1514647020\n",
      "start time 1514635020\n",
      "\n",
      "end time   1514635020\n",
      "start time 1514623020\n",
      "\n",
      "end time   1514623020\n",
      "start time 1514611020\n",
      "\n",
      "end time   1514611020\n",
      "start time 1514599020\n",
      "\n",
      "end time   1514599020\n",
      "start time 1514587020\n",
      "\n",
      "end time   1514587020\n",
      "start time 1514575020\n",
      "\n",
      "end time   1514575020\n",
      "start time 1514563020\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end time   1514563020\n",
      "start time 1514551020\n",
      "\n",
      "end time   1514551020\n",
      "start time 1514539020\n",
      "\n",
      "end time   1514539020\n",
      "start time 1514527020\n",
      "\n",
      "end time   1514527020\n",
      "start time 1514515020\n",
      "\n",
      "end time   1514515020\n",
      "start time 1514503020\n",
      "\n",
      "end time   1514503020\n",
      "start time 1514491020\n",
      "\n",
      "end time   1514491020\n",
      "start time 1514479020\n",
      "\n",
      "end time   1514479020\n",
      "start time 1514467020\n",
      "\n",
      "end time   1514467020\n",
      "start time 1514455020\n",
      "\n",
      "end time   1514455020\n",
      "start time 1514443020\n",
      "\n",
      "end time   1514443020\n",
      "start time 1514431020\n",
      "\n",
      "end time   1514431020\n",
      "start time 1514419020\n",
      "\n",
      "end time   1514419020\n",
      "start time 1514407020\n",
      "\n",
      "end time   1514407020\n",
      "start time 1514395020\n",
      "\n",
      "end time   1514395020\n",
      "start time 1514383020\n",
      "\n",
      "end time   1514383020\n",
      "start time 1514371020\n",
      "\n",
      "end time   1514371020\n",
      "start time 1514359020\n",
      "\n",
      "end time   1514359020\n",
      "start time 1514347020\n",
      "\n",
      "end time   1514347020\n",
      "start time 1514335020\n",
      "\n",
      "end time   1514335020\n",
      "start time 1514323020\n",
      "\n",
      "end time   1514323020\n",
      "start time 1514311020\n",
      "\n",
      "end time   1514311020\n",
      "start time 1514299020\n",
      "\n",
      "end time   1514299020\n",
      "start time 1514287020\n",
      "\n",
      "end time   1514287020\n",
      "start time 1514275020\n",
      "\n",
      "end time   1514275020\n",
      "start time 1514263020\n",
      "\n",
      "end time   1514263020\n",
      "start time 1514251020\n",
      "\n",
      "end time   1514251020\n",
      "start time 1514239020\n",
      "\n",
      "end time   1514239020\n",
      "start time 1514227020\n",
      "\n",
      "end time   1514227020\n",
      "start time 1514215020\n",
      "\n",
      "end time   1514215020\n",
      "start time 1514203020\n",
      "\n",
      "end time   1514203020\n",
      "start time 1514191020\n",
      "\n",
      "end time   1514191020\n",
      "start time 1514179020\n",
      "\n",
      "end time   1514179020\n",
      "start time 1514167020\n",
      "\n",
      "end time   1514167020\n",
      "start time 1514155020\n",
      "\n",
      "Retrieved: 43386\n"
     ]
    }
   ],
   "source": [
    "# Get historical data\n",
    "\n",
    "cumulative_candles = []\n",
    "minutes_in_month = 30 * 24 * 60\n",
    "\n",
    "end_time -= end_time % 60\n",
    "\n",
    "while len(cumulative_candles) < minutes_in_month:\n",
    "    start_time = end_time - 60 * 200\n",
    "    print(\"end time  \", end_time)\n",
    "    print(\"start time\", start_time)\n",
    "    print()\n",
    "    st = datetime.datetime.utcfromtimestamp(start_time)\n",
    "    et = datetime.datetime.utcfromtimestamp(end_time)\n",
    "    response = public_client.get_product_historic_rates('BTC-USD', granularity=60, start = st.isoformat(), end = et.isoformat())\n",
    "    cumulative_candles.extend(response)\n",
    "    end_time = int(cumulative_candles[-1][0])\n",
    "    time.sleep(1)\n",
    "\n",
    "print(\"Retrieved:\", len(cumulative_candles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cumulative_candles' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-a0df07d2aafd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mminutely_dataframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcumulative_candles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mminutely_dataframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'minutely.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cumulative_candles' is not defined"
     ]
    }
   ],
   "source": [
    "minutely_dataframe = pd.DataFrame(cumulative_candles, columns = columns)\n",
    "\n",
    "minutely_dataframe.to_csv('minutely.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "daily = public_client.get_product_historic_rates('BTC-USD', granularity=86400)\n",
    "\n",
    "daily_dataframe = pd.DataFrame(daily, columns = columns)\n",
    "daily_dataframe.to_csv('daily.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](http://2.bp.blogspot.com/-wuinSTn-X4A/UwHmmceDQqI/AAAAAAAAJFo/5EjPg-LpAJc/s1600/Sivakumar_Vellingiri_Normal_Forms_Poster.Jpeg \"Logo Title Text 1\")\n",
    "\n",
    "# Step 1 - Data Processing\n",
    "\n",
    "- The data is inputed as a .csv file.\n",
    "- Lets time-series transform data from (num days x num features) to (num days- window size x num days per sample x num features) where window size is 50\n",
    "- Normalization via dividing each value in the window by the first value of the window and then subtracting one.i.e [4,3,2] into [0, -0.25, -0.5]. .\n",
    "- The unnormalized bases are kept to compare the model's predictions of prices with the true prices. \n",
    "- The first 90% of the data is used in training the model, and the last 10% will be used to test the model. \n",
    "- A list of the prices before each day Y_test is drawn from will be compiled in order to generate statistics about the model's predictions\n",
    "\n",
    "======================================================================================================================\n",
    "\n",
    "The columns of data and their definitions are as follows: \n",
    "- Annual Hash Growth: Growth in the total network computations over the past 365 days\n",
    "- Block Height: The total number of blocks in the blockchain\n",
    "- Block Interval: Average amount of time between blocks\n",
    "- Block Size: The storage size of each block (i.e. megabytes)\n",
    "- BlockChain Size: The storage size of the blockchain (i.e. gigabytes)\n",
    "- Daily Blocks: Number of blocks found each day\n",
    "- Chain Value Density: The value of bitcoin's blockchain, in terms of dollars per megabyte\n",
    "- Daily Transactions: The number of transactions included in the blockchain per day\n",
    "- Difficulty: The minimum proof-of-work threshold required for a bitcoin miner to mine a block\n",
    "- Fee Percentage: Average fee paid as a percentage of transaction volume\n",
    "- Fee Rate: Average fee paid per transaction\n",
    "- Two-Week Hash Growth: Growth in the total network computations over the past 14 days\n",
    "- Hash Rate: The number of block solutions computed per second by all miners\n",
    "- Market Capitalization: The market value of all bitcoin in circulation\n",
    "- Metcalfe's Law - TX: A variant of Metcalfe's Law in which price is divided by n log n number of daily transactions\n",
    "- Metcalfe's Law - UTXO: A variant of Metcalfe's Law in which price is divided by n log n number of unspent transaction outputs\n",
    "- Miner Revenue Value: The amount of dollars earned by the mining network\n",
    "- Miner Revenue: The amount of bitcoin earned by the mining network, in the form of block rewards and transaction fees\n",
    "- Money Supply: The amount of bitcoin in circulation\n",
    "- Output Value: The dollar value of all outputs sent over the network\n",
    "- Output Volume: The amount of Bitcoin sent over the network\n",
    "- Bitcoin Price: The amount of dollars a single bitcoin is worth\n",
    "- Quarterly Hash Growth: Growth in the total network computations in the past 90 days\n",
    "- Total Transactions: The running total number of transactions processed by the Bitcoin network\n",
    "- Transaction Amount: The average amount of bitcoin moved per transaction\n",
    "- Fees Value: The dollar value of mining fees\n",
    "- Transaction Fees: The amount of bitcoin paid to miners in fees\n",
    "- Transaction Size: The average data size of a transaction\n",
    "- Transaction Value: The average dollar value moved in each transaction\n",
    "- Transactions per Block: The number of transactions in each block\n",
    "- Average UTXO Amount: The average amount of bitcoin contained in each unspent transaction output\n",
    "- UTXO Growth: The net number of unspent transaction outputs created\n",
    "- UTXO Set Size: The total number of unspent transaction outputs\n",
    "- Average UTXO Value: The average dollar value of each uspent transaction output\n",
    "- Velocity - Daily: The proportion of the money supply transacted each day\n",
    "- Velocity - Quarterly: The proportion of the money supply transacted each day, computed on a rolling-quarter basis\n",
    "- Velocity of Money: How many times the money supply changes hands in a given year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(filename, sequence_length, start = None):\n",
    "    \"\"\"\n",
    "    Loads the bitcoin data\n",
    "    \n",
    "    Arguments:\n",
    "    filename -- A string that represents where the .csv file can be located\n",
    "    sequence_length -- An integer of how many days should be looked at in a row\n",
    "    \n",
    "    Returns:\n",
    "    X_train -- A tensor of shape (2400, 49, 35) that will be inputed into the model to train it\n",
    "    Y_train -- A tensor of shape (2400,) that will be inputed into the model to train it\n",
    "    X_test -- A tensor of shape (267, 49, 35) that will be used to test the model's proficiency\n",
    "    Y_test -- A tensor of shape (267,) that will be used to check the model's predictions\n",
    "    Y_daybefore -- A tensor of shape (267,) that represents the price of bitcoin the day before each Y_test value\n",
    "    unnormalized_bases -- A tensor of shape (267,) that will be used to get the true prices from the normalized ones\n",
    "    window_size -- An integer that represents how many days of X values the model can look at at once\n",
    "    \"\"\"\n",
    "    \n",
    "    if start == None:\n",
    "        start = 0\n",
    "    \n",
    "    #Read the data file\n",
    "    raw_data = pd.read_csv(filename, nrows = 5000 ,dtype = float).values\n",
    "    \n",
    "    #Change all zeros to the number before the zero occurs\n",
    "    for x in range(0, raw_data.shape[0]):\n",
    "        for y in range(0, raw_data.shape[1]):\n",
    "            if(raw_data[x][y] == 0):\n",
    "                raw_data[x][y] = raw_data[x-1][y]\n",
    "    \n",
    "    #Convert the file to a list\n",
    "    data = raw_data.tolist()\n",
    "    #Convert the data to a 3D array (a x b x c) \n",
    "    #Where a is the number of days, b is the window size, and c is the number of features in the data file\n",
    "\n",
    "    result = []\n",
    "    for index in range(len(data) - sequence_length):\n",
    "        result.append(data[index: index + sequence_length])\n",
    "    \n",
    "#     print (result[-1], len(result))\n",
    "    #Normalizing data by going through each window\n",
    "    #Every value in the window is divided by the first value in the window, and then 1 is subtracted\n",
    "\n",
    "    d0 = np.array(result)\n",
    "#     print(d0[0])\n",
    "    dr = np.zeros_like(d0)\n",
    "    dr[:,1:,:] = d0[:,1:,:] / d0[:,0:1,:] - 1\n",
    "    \n",
    "    #Keeping the unnormalized prices for Y_test\n",
    "    #Useful when graphing bitcoin price over time later\n",
    "    end   = int(dr.shape[0])\n",
    "    unnormalized_bases = d0[start:end + 1,0:1,4]\n",
    "    \n",
    "    print(\"Total dr shape\", dr.shape)\n",
    "    \n",
    "    #Splitting data set into training (First 90% of data points) and testing data (last 10% of data points)\n",
    "    split_line = round(0.9 * dr.shape[0])\n",
    "    training_data = dr[:int(split_line), :]\n",
    "    \n",
    "    #Shuffle the data\n",
    "    np.random.shuffle(training_data)\n",
    "    \n",
    "    #Training Data\n",
    "    #the 4 is the column of the data that we want to train for\n",
    "    X_train = training_data[:, :-1]\n",
    "    Y_train = training_data[:, -1]\n",
    "    Y_train = Y_train[:, 4]\n",
    "    \n",
    "    #Testing data\n",
    "    X_test = dr[int(split_line):, :-1]\n",
    "    Y_test = dr[int(split_line):, 49, :]\n",
    "    Y_test = Y_test[:, 4]\n",
    "\n",
    "    #Get the day before Y_test's price\n",
    "    Y_daybefore = dr[int(split_line):, 48, :]\n",
    "    Y_daybefore = Y_daybefore[:, 4]\n",
    "    \n",
    "    #Get window size and sequence length\n",
    "    sequence_length = sequence_length\n",
    "    window_size = sequence_length - 1 #because the last value is reserved as the y value\n",
    "    \n",
    "    return X_train, Y_train, X_test, Y_test, Y_daybefore, unnormalized_bases, window_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Step 2 - Building the Model\n",
    "\n",
    "- We'll use a 3layer RNN with  20% dropout at each layer to reduce overfitting to the training data. \n",
    "- This model will have 515,579 trainable parameters throughout all of its layers. \n",
    "- The model uses the AdamOptimizer as its optimization function.\n",
    "- The loss function used in this model is mean squared error. \n",
    "- A linear activation function is used in this model to determine the output of each neuron in the model. The linear activation function is simply defined as f(x) = x.\n",
    "- The model will use Keras's Sequential model with Bidirectional LSTM layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](http://d3kbpzbmcynnmx.cloudfront.net/wp-content/uploads/2015/09/rnn.jpg \"Logo Title Text 1\")\n",
    "![alt text](https://docs.microsoft.com/en-us/azure/machine-learning/preview/media/scenario-tdsp-biomedical-recognition/lstm-cell.png \"Logo Title Text 1\")\n",
    "![alt text](http://d3kbpzbmcynnmx.cloudfront.net/wp-content/uploads/2015/09/bidirectional-rnn.png \"Logo Title Text 1\")\n",
    "![alt text](https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2017/05/Comparison-of-Adam-to-Other-Optimization-Algorithms-Training-a-Multilayer-Perceptron.png \"Logo Title Text 1\")\n",
    "\n",
    "\n",
    "Bidirectional RNNs are based on the idea that the output at time t may not only depend on the previous elements in the sequence, but also future elements. For example, to predict a missing word in a sequence you want to look at both the left and the right context. Bidirectional RNNs are quite simple. They are just two RNNs stacked on top of each other. The output is then computed based on the hidden state of both RNNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_model(window_size, dropout_value, activation_function, loss_function, optimizer):\n",
    "    \"\"\"\n",
    "    Initializes and creates the model to be used\n",
    "    \n",
    "    Arguments:\n",
    "    window_size -- An integer that represents how many days of X_values the model can look at at once\n",
    "    dropout_value -- A decimal representing how much dropout should be incorporated at each level, in this case 0.2\n",
    "    activation_function -- A string to define the activation_function, in this case it is linear\n",
    "    loss_function -- A string to define the loss function to be used, in the case it is mean squared error\n",
    "    optimizer -- A string to define the optimizer to be used, in the case it is adam\n",
    "    \n",
    "    Returns:\n",
    "    model -- A 3 layer RNN with 100*dropout_value dropout in each layer that uses activation_function as its activation\n",
    "             function, loss_function as its loss function, and optimizer as its optimizer\n",
    "    \"\"\"\n",
    "    #Create a Sequential model using Keras\n",
    "    model = Sequential()\n",
    "\n",
    "    #First recurrent layer with dropout\n",
    "    model.add(Bidirectional(LSTM(window_size, return_sequences=True), input_shape=(window_size, X_train.shape[-1]),))\n",
    "    model.add(Dropout(dropout_value))\n",
    "\n",
    "    #Second recurrent layer with dropout\n",
    "    model.add(Bidirectional(LSTM((window_size*2), return_sequences=True)))\n",
    "    model.add(Dropout(dropout_value))\n",
    "\n",
    "    #Third recurrent layer\n",
    "    model.add(Bidirectional(LSTM(window_size, return_sequences=False)))\n",
    "\n",
    "    #Output layer (returns the predicted value)\n",
    "    model.add(Dense(units=1))\n",
    "    \n",
    "    #Set activation function\n",
    "    model.add(Activation(activation_function))\n",
    "\n",
    "    #Set loss function and optimizer\n",
    "    model.compile(loss=loss_function, optimizer=optimizer)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3 - Training the Model\n",
    "\n",
    "- The model will be fitted to the training dat with a batch_size of 1024. \n",
    "- Additionally, 100 epochs will be performed to give the model time to adjust its weights and biases to fit the training data.\n",
    "- 5% of the training data will be used as the validation set.  \n",
    "- The model will train by minimizing the loss (mean squared error) of its training data. -- - The validation set is useful when attempting to identify signs of overfitting. \n",
    "- If the validation loss begins to consistently and rapidly increase, the model has overfitted to the training data, and changes should be made to the model. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def fit_model(model, X_train, Y_train, batch_num, num_epoch, val_split):\n",
    "    \"\"\"\n",
    "    Fits the model to the training data\n",
    "    \n",
    "    Arguments:\n",
    "    model -- The previously initalized 3 layer Recurrent Neural Network\n",
    "    X_train -- A tensor of shape (2400, 49, 35) that represents the x values of the training data\n",
    "    Y_train -- A tensor of shape (2400,) that represents the y values of the training data\n",
    "    batch_num -- An integer representing the batch size to be used, in this case 1024\n",
    "    num_epoch -- An integer defining the number of epochs to be run, in this case 100\n",
    "    val_split -- A decimal representing the proportion of training data to be used as validation data\n",
    "    \n",
    "    Returns:\n",
    "    model -- The 3 layer Recurrent Neural Network that has been fitted to the training data\n",
    "    training_time -- An integer representing the amount of time (in seconds) that the model was training\n",
    "    \"\"\"\n",
    "    #Record the time the model starts training\n",
    "    start = time.time()\n",
    "\n",
    "    #Train the model on X_train and Y_train\n",
    "    model.fit(X_train, Y_train, batch_size= batch_num, nb_epoch=num_epoch, validation_split= val_split)\n",
    "\n",
    "    #Get the time it took to train the model (in seconds)\n",
    "    training_time = int(math.floor(time.time() - start))\n",
    "    return model, training_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4 - Testing the Model\n",
    "\n",
    "- The models given x values of testing data & will predict normalized prices (y_predict)\n",
    "- Then, both the predicted values and the real values will be unnormalized and stored in separate arrays. \n",
    "- The values are unnormalized by looping through the predicted and true values. \n",
    "- 1 is added to each value, and then the result is multiplied by a corresponding number in the unnormalized_bases array. \n",
    "- In other words, the unnormalization processs is the exact reverse of the normalization process\n",
    "- Finally, a plot is created of the unnormalized real values and the unnormalized predicted values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def test_model(model, X_test, Y_test, unnormalized_bases):\n",
    "    \"\"\"\n",
    "    Test the model on the testing data\n",
    "    \n",
    "    Arguments:\n",
    "    model -- The previously fitted 3 layer Recurrent Neural Network\n",
    "    X_test -- A tensor of shape (267, 49, 35) that represents the x values of the testing data\n",
    "    Y_test -- A tensor of shape (267,) that represents the y values of the testing data\n",
    "    unnormalized_bases -- A tensor of shape (267,) that can be used to get unnormalized data points\n",
    "    \n",
    "    Returns:\n",
    "    y_predict -- A tensor of shape (267,) that represnts the normalized values that the model predicts based on X_test\n",
    "    real_y_test -- A tensor of shape (267,) that represents the actual prices of bitcoin throughout the testing period\n",
    "    real_y_predict -- A tensor of shape (267,) that represents the model's predicted prices of bitcoin\n",
    "    fig -- A branch of the graph of the real predicted prices of bitcoin versus the real prices of bitcoin\n",
    "    \"\"\"\n",
    "\n",
    "    #Test the model on X_Test\n",
    "    y_predict = model.predict(X_test)\n",
    "\n",
    "    #Create empty 2D arrays to store unnormalized values\n",
    "    real_y_test = np.zeros_like(Y_test)\n",
    "    real_y_predict = np.zeros_like(y_predict)\n",
    "\n",
    "    #Fill the 2D arrays with the real value and the predicted value by reversing the normalization process\n",
    "    for i in range(Y_test.shape[0]):\n",
    "        y = Y_test[i]\n",
    "        predict = y_predict[i]\n",
    "        real_y_test[i] = (y+1)*unnormalized_bases[i]\n",
    "        real_y_predict[i] = (predict+1)*unnormalized_bases[i]\n",
    "\n",
    "    #Plot of the predicted prices versus the real prices\n",
    "    fig = plt.figure(figsize=(10,5))\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.set_title(\"Bitcoin Price Over Time\")\n",
    "    plt.plot(real_y_predict, color = 'green', label = 'Predicted Price')\n",
    "    plt.plot(real_y_test, color = 'red', label = 'Real Price')\n",
    "    ax.set_ylabel(\"Price (USD)\")\n",
    "    ax.set_xlabel(\"Time (Minutes)\")\n",
    "    ax.legend()\n",
    "    \n",
    "    return y_predict, real_y_test, real_y_predict, fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5 - Evaluating Change in Price\n",
    "\n",
    "- Lets plot the model's predicted change in price each day against the real change in price daily\n",
    "- The percent increases of the predicted values and the real values are calculated by subtracting the value from the day before from the predicted/real value then dividing the result by 1+the value from the day before. \n",
    "- The predicted change in price is stored in delta_predict, while the real change in price is stored in delta_real.\n",
    "- These two tensors are then graphed together to visualize the difference between predicted and real change in price for bitcoin throughout the testing period. \n",
    "- The plot will represent the percent change in bitcoin price each day. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def price_change(Y_daybefore, Y_test, y_predict):\n",
    "    \"\"\"\n",
    "    Calculate the percent change between each value and the day before\n",
    "    \n",
    "    Arguments:\n",
    "    Y_daybefore -- A tensor of shape (267,) that represents the prices of each day before each price in Y_test\n",
    "    Y_test -- A tensor of shape (267,) that represents the normalized y values of the testing data\n",
    "    y_predict -- A tensor of shape (267,) that represents the normalized y values of the model's predictions\n",
    "    \n",
    "    Returns:\n",
    "    Y_daybefore -- A tensor of shape (267, 1) that represents the prices of each day before each price in Y_test\n",
    "    Y_test -- A tensor of shape (267, 1) that represents the normalized y values of the testing data\n",
    "    delta_predict -- A tensor of shape (267, 1) that represents the difference between predicted and day before values\n",
    "    delta_real -- A tensor of shape (267, 1) that represents the difference between real and day before values\n",
    "    fig -- A plot representing percent change in bitcoin price per day,\n",
    "    \"\"\"\n",
    "    #Reshaping Y_daybefore and Y_test\n",
    "    Y_daybefore = np.reshape(Y_daybefore, (-1, 1))\n",
    "    Y_test = np.reshape(Y_test, (-1, 1))\n",
    "\n",
    "    #The difference between each predicted value and the value from the day before\n",
    "    delta_predict = (y_predict - Y_daybefore) / (1+Y_daybefore)\n",
    "\n",
    "    #The difference between each true value and the value from the day before\n",
    "    delta_real = (Y_test - Y_daybefore) / (1+Y_daybefore)\n",
    "\n",
    "    #Plotting the predicted percent change versus the real percent change\n",
    "    fig = plt.figure(figsize=(10, 6))\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.set_title(\"Percent Change in Bitcoin Price Per Day\")\n",
    "    plt.plot(delta_predict, color='green', label = 'Predicted Percent Change')\n",
    "    plt.plot(delta_real, color='red', label = 'Real Percent Change')\n",
    "    plt.ylabel(\"Percent Change\")\n",
    "    plt.xlabel(\"Time (Days)\")\n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    return Y_daybefore, Y_test, delta_predict, delta_real, fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6 - Process the Percent Change in Price\n",
    "\n",
    "- The percent change in price will be processsed such that an increase in price is represented by a 1, and a decrease/no change is represented by a 0. These binary values will be stored in arrays delta_predict_1_0 and delta_real_1_0. \n",
    "\n",
    "- This will be done by looping through the values of the real and predicted percent change arrays. If a value is greater than 0, a 1 is stored in a new array. Otherwise, a 0 is stored in the new array.\n",
    "\n",
    "- This process is very useful to understand how well the model did, and can be used to gather statistics about the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def binary_price(delta_predict, delta_real):\n",
    "    \"\"\"\n",
    "    Converts percent change to a binary 1 or 0, where 1 is an increase and 0 is a decrease/no change\n",
    "    \n",
    "    Arguments:\n",
    "    delta_predict -- A tensor of shape (267, 1) that represents the predicted percent change in price\n",
    "    delta_real -- A tensor of shape (267, 1) that represents the real percent change in price\n",
    "    \n",
    "    Returns:\n",
    "    delta_predict_1_0 -- A tensor of shape (267, 1) that represents the binary version of delta_predict\n",
    "    delta_real_1_0 -- A tensor of shape (267, 1) that represents the binary version of delta_real\n",
    "    \"\"\"\n",
    "    #Empty arrays where a 1 represents an increase in price and a 0 represents a decrease in price\n",
    "    delta_predict_1_0 = np.empty(delta_predict.shape)\n",
    "    delta_real_1_0 = np.empty(delta_real.shape)\n",
    "\n",
    "    #If the change in price is greater than zero, store it as a 1\n",
    "    #If the change in price is less than zero, store it as a 0\n",
    "    for i in range(delta_predict.shape[0]):\n",
    "        if delta_predict[i][0] > 0:\n",
    "            delta_predict_1_0[i][0] = 1\n",
    "        else:\n",
    "            delta_predict_1_0[i][0] = 0\n",
    "    for i in range(delta_real.shape[0]):\n",
    "        if delta_real[i][0] > 0:\n",
    "            delta_real_1_0[i][0] = 1\n",
    "        else:\n",
    "            delta_real_1_0[i][0] = 0    \n",
    "\n",
    "    return delta_predict_1_0, delta_real_1_0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7 - Comparing Predictions and Real Data\n",
    "\n",
    "The binary categories computed in the previous cell is now used to compare predicted and real data. It will be used to find the number of:\n",
    "- True positives\n",
    "- False positives\n",
    "- True negatives\n",
    "- False negatives\n",
    "These can then be used to further calculate statistics of the model's performance. \n",
    "\n",
    "This will be done by looping through both binary arrays at once and getting the corresponding values. If the real value is a 1 and the predicted value is a 1, that index will be counted as a true positive. If the real value is a 1 and the predicted value is a 0, that index will be counted as a false negative. If the real value is a 0 and the predicted value is a 0, that index will be counted as a true negative. If the real value is a 0 and the predicted value is a 1, that index will be counted as a false positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_positives_negatives(delta_predict_1_0, delta_real_1_0):\n",
    "    \"\"\"\n",
    "    Finding the number of false positives, false negatives, true positives, true negatives\n",
    "    \n",
    "    Arguments: \n",
    "    delta_predict_1_0 -- A tensor of shape (267, 1) that represents the binary version of delta_predict\n",
    "    delta_real_1_0 -- A tensor of shape (267, 1) that represents the binary version of delta_real\n",
    "    \n",
    "    Returns:\n",
    "    true_pos -- An integer that represents the number of true positives achieved by the model\n",
    "    false_pos -- An integer that represents the number of false positives achieved by the model\n",
    "    true_neg -- An integer that represents the number of true negatives achieved by the model\n",
    "    false_neg -- An integer that represents the number of false negatives achieved by the model\n",
    "    \"\"\"\n",
    "    #Finding the number of false positive/negatives and true positives/negatives\n",
    "    true_pos = 0\n",
    "    false_pos = 0\n",
    "    true_neg = 0\n",
    "    false_neg = 0\n",
    "    for i in range(delta_real_1_0.shape[0]):\n",
    "        real = delta_real_1_0[i][0]\n",
    "        predicted = delta_predict_1_0[i][0]\n",
    "        if real == 1:\n",
    "            if predicted == 1:\n",
    "                true_pos += 1\n",
    "            else:\n",
    "                false_neg += 1\n",
    "        elif real == 0:\n",
    "            if predicted == 0:\n",
    "                true_neg += 1\n",
    "            else:\n",
    "                false_pos += 1\n",
    "    return true_pos, false_pos, true_neg, false_neg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8 - Calculating Statistics\n",
    "\n",
    "![alt text](https://www.researchgate.net/profile/Alexandros_Karatzoglou/publication/221515860/figure/fig1/AS:339586132791298@1457975051470/Figure-1-Mean-Squared-Error-formula-used-to-evaluate-the-user-model.ppm \"Logo Title Text 1\")\n",
    "\n",
    "![alt text](https://image.slidesharecdn.com/qconrio-machinelearningforeveryone-150826200704-lva1-app6892/95/qcon-rio-machine-learning-for-everyone-51-638.jpg?cb=1440698161 \"Logo Title Text 1\")\n",
    "\n",
    "\n",
    "Putting everything together and getting statistics about the model. Statistics being calculated include:\n",
    "- Precision: How often the model gets a true positive compared to how often it returns a positive\n",
    "- Recall: How often the model gets a true positive compared to how often it should have gotten a positive\n",
    "- F1 Score: The weighted average of recall and precision\n",
    "- Mean Squared Error: The average of the squares of the differences between predicted and real values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_statistics(true_pos, false_pos, true_neg, false_neg, y_predict, Y_test):\n",
    "    \"\"\"\n",
    "    Calculate various statistics to assess performance\n",
    "    \n",
    "    Arguments:\n",
    "    true_pos -- An integer that represents the number of true positives achieved by the model\n",
    "    false_pos -- An integer that represents the number of false positives achieved by the model\n",
    "    true_neg -- An integer that represents the number of true negatives achieved by the model\n",
    "    false_neg -- An integer that represents the number of false negatives achieved by the model\n",
    "    Y_test -- A tensor of shape (267, 1) that represents the normalized y values of the testing data\n",
    "    y_predict -- A tensor of shape (267, 1) that represents the normalized y values of the model's predictions\n",
    "    \n",
    "    Returns:\n",
    "    precision -- How often the model gets a true positive compared to how often it returns a positive\n",
    "    recall -- How often the model gets a true positive compared to how often is hould have gotten a positive\n",
    "    F1 -- The weighted average of recall and precision\n",
    "    Mean Squared Error -- The average of the squares of the differences between predicted and real values\n",
    "    \"\"\"\n",
    "    precision = float(true_pos) / (true_pos + false_pos)\n",
    "    recall = float(true_pos) / (true_pos + false_neg)\n",
    "    F1 = float(2 * precision * recall) / (precision + recall)\n",
    "    #Get Mean Squared Error\n",
    "    MSE = mean_squared_error(y_predict.flatten(), Y_test.flatten())\n",
    "\n",
    "    return precision, recall, F1, MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9 - Putting It All Together\n",
    "\n",
    "Applying all the methods defined above and analyzing results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total dr shape (4950, 50, 6)\n",
      "(4455, 49, 6)\n",
      "(4455,)\n",
      "(495, 49, 6)\n",
      "(495,)\n",
      "(495,)\n",
      "(4950, 1)\n",
      "49\n"
     ]
    }
   ],
   "source": [
    "X_train, Y_train, X_test, Y_test, Y_daybefore, unnormalized_bases, window_size = load_data(\"./minutely.csv\", 50)\n",
    "print (X_train.shape)\n",
    "print (Y_train.shape)\n",
    "print (X_test.shape)\n",
    "print (Y_test.shape)\n",
    "print (Y_daybefore.shape)\n",
    "print (unnormalized_bases.shape)\n",
    "print (window_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional_25 (Bidirectio (None, 49, 98)            21952     \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 49, 98)            0         \n",
      "_________________________________________________________________\n",
      "bidirectional_26 (Bidirectio (None, 49, 196)           154448    \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 49, 196)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_27 (Bidirectio (None, 98)                96432     \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1)                 99        \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 272,931\n",
      "Trainable params: 272,931\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = initialize_model(window_size, 0.25, 'linear', 'mse', 'adam')\n",
    "print (model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maisam/.pyenv/versions/anaconda3-5.0.0/lib/python3.6/site-packages/keras/models.py:939: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4232 samples, validate on 223 samples\n",
      "Epoch 1/50\n",
      "4232/4232 [==============================] - 26s 6ms/step - loss: 2.1330e-04 - val_loss: 1.7228e-04\n",
      "Epoch 2/50\n",
      "4232/4232 [==============================] - 24s 6ms/step - loss: 1.9984e-04 - val_loss: 1.5246e-04\n",
      "Epoch 3/50\n",
      "2048/4232 [=============>................] - ETA: 13s - loss: 1.7792e-04"
     ]
    }
   ],
   "source": [
    "model, training_time = fit_model(model, X_train, Y_train, 1024, 50, .05)\n",
    "\n",
    "#Print the training time\n",
    "print (\"Training time\", training_time, \"seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict, real_y_test, real_y_predict, fig1 = test_model(model, X_test, Y_test, unnormalized_bases)\n",
    "\n",
    "#Show the plot\n",
    "plt.show(fig1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting Percent Change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_daybefore, Y_test, delta_predict, delta_real, fig2 = price_change(Y_daybefore, Y_test, y_predict)\n",
    "\n",
    "#Show the plot\n",
    "plt.show(fig2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Binary Version of Percent Change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_predict_1_0, delta_real_1_0 = binary_price(delta_predict, delta_real)\n",
    "\n",
    "print (delta_predict_1_0.shape)\n",
    "print (delta_real_1_0.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Predictions and True Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_pos, false_pos, true_neg, false_neg = find_positives_negatives(delta_predict_1_0, delta_real_1_0)\n",
    "print (\"True positives:\", true_pos)\n",
    "print (\"False positives:\", false_pos)\n",
    "print (\"True negatives:\", true_neg)\n",
    "print (\"False negatives:\", false_neg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "precision, recall, F1, MSE = calculate_statistics(true_pos, false_pos, true_neg, false_neg, y_predict, Y_test)\n",
    "print( \"Precision:\", precision)\n",
    "print( \"Recall:\", recall)\n",
    "print( \"F1 score:\", F1)\n",
    "print( \"Mean Squared Error:\", MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
